%!TEX root = ../main.tex
We want to use this theory to justify more formally the method we introduced.

Let's start under the hypothesis that our model class is rich enough to describe our system.
\begin{equation*}
S\in \Mc \quad \left(\text{i.e. }\exists \theta ^{0} \in \Theta :S\equiv y(t) =\frac{B\left(z,\theta ^{0}\right)}{A\left(z,\theta ^{0}\right)} u(t-d) +\frac{C\left(z,\theta ^{0}\right)}{A\left(z,\theta ^{0}\right)} e(t) ,e(t) \sim \WN \left(0,\lambda ^{2}\right)\right)
\end{equation*}
Our model is not just a model, but the true system through which $ y(t)$ is generated.

The expression of the asymptotic cost function is:
\begin{equation*}
\begin{aligned}
\bar{J}(\theta) & =\E\left[ \varepsilon (t,\theta)^{2}\right] =\E\left[(y(t) -\hat{y}(t\mid t-1,\theta))^{2}\right] & \\
 & =\E\left[\left(y(t) \pm \hat{y}\left(t\mid t-1,\theta ^{0}\right) -\hat{y}(t\mid t-1,\theta)\right)^{2}\right] & \\
 & =\E\left[\left(\textcolor[rgb]{1,0,0}{y(t) -\hat{y}\left(t\mid t-1,\theta^{0}\right)} +\hat{y}\left(t\mid t-1,\theta ^{0}\right) -\hat{y}(t\mid t-1,\theta)\right)^{2}\right] &  \begin{array}{l}
\textcolor[rgb]{1,0,0}{(*)}\text{:generated by } S=\Mc\left(\theta ^{0}\right)\\
\text{optimal 1-step pred. error is the } \WN
\end{array}\\
 & =\E\left[\left(e^{0}(t) +\hat{y}\left(t\mid t-1,\theta ^{0}\right) -\hat{y}(t\mid t-1,\theta)\right)^{2}\right] &  \begin{array}{l}
\hat{y} \text{ linear functions of } y(t-1),y(t-2) \ldots\\
\text{uncorrelated with } e^{0}(t)
\end{array}\\
 & =\E\left[ e^{0}(t)^{2}\right] +\E\left[\left(\hat{y}\left(t\mid t-1,\theta ^{0}\right) -\hat{y}(t\mid t-1,\theta)\right)^{2}\right] & \\
 & =\underbrace{\lambda {^{0}}^{2}}_{\text{constant}} +\underbrace{\E\left[\left(\hat{y}\left(t\mid t-1,\theta ^{0}\right) -\hat{y}(t\mid t-1,\theta)\right)^{2}\right]}_{\geq 0,\ =0\text{ if } \theta =\theta ^{0}} & 
\end{aligned}
\end{equation*}

So that
\begin{equation*}
\bar{J}\left(\theta ^{0}\right) = \lambda {^{0}}^{2} \leq \bar{J}(\theta)\  \forall \theta \quad\Longrightarrow\quad \theta ^{0} \text{ is a minimum of } \bar{J}(\theta)
\end{equation*}
Using \gls{pem} asymptotic theory
\begin{equation*}
\hat{\theta }_{N}\xrightarrow{N\rightarrow \infty} \Delta ,\quad\theta ^{0} \in \Delta 
\end{equation*}
If $ \Delta =\left\{\theta ^{0}\right\}$ is a singleton (i.e. $ \bar{J}$ has unique minimum) then:
\begin{align*}
	\hat{\theta }_{N} & \xrightarrow{N\rightarrow \infty} \theta ^{0}\\
	\Mc(\hat{\theta }_{N}) & \xrightarrow{N\rightarrow \infty} S
\end{align*}
Thus, the \gls{pem} identification is \textbf{asymptotically consistent}: if $ \Mc$ is rich enough and the number of collected data is big enough, \gls{pem} identification provides a good description of $ S$.



\section{Model order selection}

The identification of model consists of 4 steps:
\begin{enumerate}
\item data collection;
\item choice of $ \Mc$;
\item optimal model selection\footnote{\gls{pem}: $ \hat{\theta }_{N} =\underset{\theta \in \Theta }{\argmin} J_{N}(\theta)$};
\item model validation\footnote{evaluate the performance of $\Mc(\hat{\theta }_{N})$}.
\end{enumerate}

The process is not linear: it is common to loop from model validation to another choice of $ \Mc$ (if the model is not satisfactory) or even to data collection.

An important choice for $ \Mc$ is the model order selection. Given an ARMAX model (black-box case):
\begin{gather*}
\begin{aligned}
y(t)  & =\frac{B(z,\theta)}{A(z,\theta)} u(t-d) +\frac{1}{A(z,\theta)} e(t) & e(t) \sim \WN \left(0,\lambda ^{2}\right)\\
A(z,\theta)  & =1-a_{1} z^{-1} -\cdots -a_{m} z^{-m} & \\
B(z,\theta) & =b_{0} +b_{1} z^{-1} +\cdots +b_{p} z^{-p} & \\
C(z,\theta) & =1+c_{1} z^{-1} +\cdots +c_{n} z^{-n} & 
\end{aligned}\\
\end{gather*}
The model order is: $ (m,p,n)$, the delay $ d$ between $ I/O$ is neglected as it can be easily retrieved from data.

Notice that some trivial choices for model selection change the structure of the model:
\begin{equation*}
\begin{cases}
n=0 & \text{ARX}\\
n=0,p=0 & \text{AR}\\
n\neq 0,p=0 & \text{ARMA}
\end{cases}
\end{equation*}
However, we will deal with the case where the structure is fixed ($ n,m,p\neq 0\Longrightarrow$ ARMAX) and we just want to select a proper number. To keep the notation simple we will focus on one hyper-parameter ($ m=n=p$) without loss of generalization.

A naive idea is to use $ J_{N}$ as an indicator for model quality, selecting a maximum value for the model order. Cycling for $ m=1:\overline{m}$, where $\overline{m}$ is the maximum model order we allow for:

$ \mathtt{For }\ m=1:\overline{m}$
\begin{equation*}
\begin{aligned}
\Mc^{m} & =ARMAX(m) =\left\{\Mc(\theta) ,\theta \in \Theta ^{m}\right\}\\
\hat{\theta }_{N}^{m} & =\underset{\theta \in \Theta ^{m}}{\argmin}\frac{1}{N}\sum _{i=1}^{N}(y(i) -\hat{y}(i\mid i-1,\theta))^{2}
\end{aligned}
\end{equation*}
$ \mathtt{END}$

At the end of the cycle we select:
\begin{equation*}
\hat{\theta }_{N} =\hat{\theta }_{N}^{m} \text{ with lowest } J_{N}\left(\hat{\theta }_{N}^{m}\right)
\end{equation*}
\begin{obs}
	This naive idea does NOT work in practice: $ J_{N}\left(\hat{\theta }_{N}^{m}\right)$ is always decreasing with $ m$, so that the highest possible order is selected. As $m$ increases, the model runs in the problem of over-fitting: it's perfectly good for describing the data collected but it does not generalizes to new data. The model has overall a bad predictive performance on new data.
\end{obs}
\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.5\linewidth]{Screenshot (33)}
    \caption{Example of function interpolation.}
\end{figure}
\FloatBarrier
A very simple but powerful idea to solve the problem is to perform \textbf{cross validation}.

Cross validation consists in evaluating the performance of an identified model on new data: the dataset is divided in two parts, using the first (\textbf{training dataset}) for selecting the parameters and the second (\textbf{validation dataset}) for cross validation.

The for-loop becomes:

$ \mathtt{For } \ m=1:\overline{m}$
\begin{gather*}
\Mc^{m} =\text{ARMAX}(m) =\left\{\Mc(\theta) ,\theta \in \Theta ^{m}\right\}\\
\hat{\theta }_{T}^{m} =\underset{\theta \in \Theta ^{m}}{\argmin} J_{T}(\theta) =\underset{\theta \in \Theta ^{m}}{\argmin}\frac{1}{M}\sum _{i=1}^{M}(y(i) -\hat{y}(i\mid i-1,\theta))^{2}\\
\text{evaluate } J_{V}\left(\hat{\theta }_{T}^{m}\right) =\frac{1}{N-M}\sum _{i=M+1}^{N}\left(y(i) -\hat{y}\left(i\mid i-1,\hat{\theta }_{T}^{m}\right)\right)^{2}
\end{gather*}
$ \mathtt{END}$

Where $ J_{V}$ is the performance of the model obtained from the training set evaluated on the validation dataset. Then we select
\begin{equation*}
m^{\star} =\underset{m}{\argmin} J_{V}\left(\hat{\theta }_{T}^{m}\right) ,\hat{\theta }_{N} =\hat{\theta }_{T}^{m^{\star}}
\end{equation*}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=.8\linewidth]{Screenshot(32)}
\end{figure}
\FloatBarrier
The drawback of cross-validation is that the dataset has to be split: less information is used to tune the model and data collection can be expensive.


\subsection{Alternative approach: direct model order penalization}
The structure is similar, we run a cycle up to a maximum model order

$ \mathtt{For } \ m=1:\overline{m}$
\begin{equation*}
\begin{aligned}
\Mc^{m} & =\text{ARMAX}(m) =\left\{\Mc(\theta) ,\theta \in \Theta ^{m}\right\} & \\
\hat{\theta }_{N}^{m}  & =\underset{\theta }{\argmin}\frac{1}{N}\sum _{i=1}^{N}(y(i) -\hat{y}(i\mid i-1,\theta))^{2} & \text{all data are used to tune the model}\\
\text{evaluate } V(m) & =V(m,J_{N}(\hat{\theta }_{N}^{m})),\forall m & \text{model cost with model order penalization}
\end{aligned}
\end{equation*}
$ \mathtt{END}$

Then we select
\begin{equation*}
m^{\star} =\underset{m}{\argmin} V(m) ,\hat{\theta }_{N} =\hat{\theta }_{T}^{m^{\star}}
\end{equation*}
Some common choice are
\begin{align*}
\text{FPE} & =\text{Final Prediction Error} & \text{(probability)}\\
\text{AIC} & =\text{Akaike's Identification Criterion} & \text{(statistics)}\\
\text{MDL} & =\text{Minimum Description Length} & \text{(information theory)}
\end{align*}
\begin{equation*}
	\begin{array}{rcc}
		\text{FPE}(m) = & \frac{N+m}{N-m}          &  \cdot J_{N}\left(\hat{\theta }_{N}^{m}\right)\\
		\text{AIC}(m) = & 2\frac{m}{N}             &  +\ln\left(J_{N}(\hat{\theta }_{N}^{m})\right)\\
		\text{MDL}(m) = & \ln(N)\frac{m}{N}        &  +\ln\left(J_{N}(\hat{\theta }_{N}^{m})\right)\\
		                & \text{increasing with }m &  \text{decreasing with }m
	\end{array}
\end{equation*}
In fact, FPE and AIC are approximately equivalent:
\begin{align*}
\ln(\text{FPE}) & =\ln\left(\frac{N+m}{N-m} J_{N}(\hat{\theta }_{N}^{m})\right) \\
 & =\ln\left(\frac{1+\frac{m}{N}}{1-\frac{m}{N}} J_{N}(\hat{\theta }_{N}^{m})\right) \\
 & =\ln\left(1+\frac{m}{N}\right) -\ln\left(1-\frac{m}{N}\right) +\ln\left(J_{N}(\hat{\theta }_{N}^{m})\right) \quad m\ll N, \frac{m}{N} \approx 0\\
 & \approx \frac{m}{N} -\left(-\frac{m}{N}\right) +\ln\left(J_{N}(\hat{\theta }_{N}^{m})\right) \\
 & =2\frac{m}{N} +\ln\left(J_{N}(\hat{\theta }_{N}^{m})\right) = \text{AIC} & 
\end{align*}
MDL is equal to AIC with 2 replaced by $ \ln(N)$. Since typically $ \ln(N)  >2$ (i.e. dataset has more than 8 data points) MDL penalizes model order more than AIC and FPE.