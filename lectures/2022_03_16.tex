%!TEX root = ../main.tex

What we did \eqref{eq:reconstruction-noise-from-data} is indeed possibile (meaning we can reconstruct $e(t)$ from $y(t)$) thanks to the further assumptions we made at page \pageref{further-assumptions-prediction-theory}.


% blablabla

\begin{align*}
	\hat y ^{0}(t+k\mid t)&=\frac{F(z)}{A(z)}\left[ \frac{A(z)}{C(z)}y(t) \right]  \\
	&=\frac{F(z)}{\cancel{A(z)}}\cdot\frac{\cancel{A(z)}}{C(z)}y(t)\\
	&=\frac{F(z)}{C(z)}y(t) \quad \text{where }\frac{C(z)}{A(z)}=E(z)+\frac{z^{-k}F(z) }{A(z)}
\end{align*}
optimal predictor from ouput obtained as output of a digital filter $F/C$ fed by $y$ up to time $t$.

\emph{Example. (MA$1$)}
Let $e(t)\sim \WN(0,1)$
\begin{align*}
	y(t) &= e(t) - 2 e(t-1)\\
	&= (1-2z^{-1} )e(t) &\text{not canonical, zeros: }\{2\}\\
	&=\left( 1-\frac{1}{2} z^{-1}  \right) \cdot\frac{1-2z^{-1}}{1-\frac{1}{2} z^{-1}} e(t)\\
	&=\left( 1-\frac{1}{2} z^{-1}  \right) \xi(t) &\text{canonical}
\end{align*}
where $\xi(t)\sim \WN(0,4)$.
\[
	y(t+1)=\underbrace{e(t+1)}_{\text{unpredictable}}-\underbrace{2e(t)}_{\text{predictable at }t}
\]
then
\[
	\hat y ^{e}(t+1\mid t)=-2e(t) \quad \text{optimal predictor from measurements of $e$ up to time $t$} 
\]
One can write
\[
	y(t+1)=\underbrace{\xi(t+1)}_{\text{unpredictable}}-\underbrace{\frac{1}{2} \xi(t)}_{\text{predictable at }t}
\]
then
\[
	\hat y^{\xi} (t+1\mid t)=-\frac{1}{2} \xi(t) \quad \text{optimal predictor from measurements of $\xi$ up to time $t$} 
\]
\[
	\hat y^{e}(t+1\mid t)=-2e(t) 
\]
% rec


Prediction error
\[
	\E\left[ \left( y(t+1)-\hat y^{e}(t+1\mid t)  \right) ^2   \right]  =\E[e(t+1)^2 ]=1\neq 4=\E[\xi(t+1)^2 ]=\E\left[ \left( y(t+1)-\hat y ^{\xi}(t+1\mid t)  \right)^2    \right]  
\]
$\hat y^{e} $ and $\hat y^{\xi} $ cannot be simultaneously the predictor from output.
\todo{canonical?}

\textbf{Observation.}

\begin{align*}
	y(t+1)&=e(t+1)-2e(t)\\
	e(t)&=-\frac{1}{2} y(t+1)+\frac{1}{2} e(t+1)\\
	&= \ldots \text{recursion}\ldots \\
	&= -\frac{1}{2} y(t+1)-\frac{1}{2} y(t+2)+\frac{1}{4}e(t+2)\\
	&= -\frac{1}{2} y(t+1)-\frac{1}{4}y(t+2)-\frac{1}{8}y(t+3)+\frac{1}{8}e(t+3)\\
	&\vdots
\end{align*}
Converges.

$e(t)$ is the now canonical form \todo{what's this shit?}
It is instead reconstructible from the future of $y$, it's not suitable to compute the optimal predictor from output (by definition one must use the measurements from output up to time $t$ only).

\textbf{Observation.}
\begin{align*}
	y(t)&=e(t)-2e(t-1)\\
	e(t)&=y(t)+2e(t-1)\\
	&=y(t)+2(y(t-1)+2e(t-2))\\
	&=y(t)+2y(t-1)+2e(t-2)\\
	&\vdots
\end{align*}
Diverges.
\todo{saltone incredibile fino a}


\section{Optimal prediction of non zero mean ARMA}
\[
	y(t)=\frac{C(z)}{A(z)}e(t)\qquad e(t)\sim \WN(\mu,\lambda^2 )
\]
Hypothesis: canonical and minimum phase.
$\hat y(t+k\mid t)=?$
\[
	\E[e(t)]=\mu \implies \E[y(t)]=\frac{C(1)}{A(1)}\cdot\mu=m_{y} \quad \forall t
\]
We construct
\[
	\begin{cases}
		\tilde y(t)=y(t)-m_{y}\\
		\tilde e(t)=e(t)-\mu
	\end{cases}
	\implies 
	\tilde y(t)=\frac{C(z)}{A(z)}\tilde e(t) \quad \tilde e(t)\sim \WN(0,\lambda^2)
\]
$\tilde y$ is a zero mean ARMA
\[
	\hat{\tilde y} (t+k\mid t) = \frac{F(z)}{C(z)}\tilde y(t) \qquad \frac{C(z)}{A(z)}=E(z)+\frac{z^{-k}F(z) }{A(z)}
\]
So that we have
\[
	y(t)=\tilde y(t)+m_{y} \qquad y(t+k)=\tilde y(t+k)+\underbrace{m_{y}}_{\text{known}}
\]
\begin{align*}
	\hat y(t+k\mid t)&=\widehat{\tilde y(t+k)+m_{y}}\\
	&= \hat{\tilde y}(t+k\mid t)+m_{y}\\
	&=\frac{F(z)}{C(z)}\tilde y(t)+m_{y}\\
	&=\frac{F(z)}{C(z)}(y(t)-m_{y})+m_{y}\\
	&=\frac{F(z)}{C(z)}y(t)-\frac{F(z)}{C(z)}m_{y}+m_{y}\\
	&=\frac{F(z)}{C(z)}y(t)+\left( 1-\frac{F(1)}{C(1)} \right)  \cdot m_{y}
\end{align*}
\todo{passaggio non chiaro}

ARMA X
\[
	y(t)=\frac{B(z)}{A(z)} u(t-a)+\frac{C(z)}{A(z)}e(t)\qquad e(t)\sim \WN(0,\lambda^2 )
\]
available information:
\begin{gather*}
	y(t),y(t-1),\ldots \\
	u(t),u(t-1),\ldots
\end{gather*}































