%!TEX root = ../main.tex

What we did \eqref{eq:reconstruction-noise-from-data} is indeed possibile (meaning we can reconstruct $e(t)$ from $y(t)$) thanks to the further assumptions we made at page \pageref{further-assumptions-prediction-theory}. In fact, this means that $W(z)^{-1}=\frac{A(z)}{C(z)}$ is asymptotically stable too.

From the point of view of transfer functions we have that:
\[
	\hat y (t+k\mid t) = \frac{F(z)}{A(z)}e(t) =\frac{F(z)}{\cancel{A(z)}}\cdot\frac{\cancel{A(z)}}{C(z)}y(t) =\frac{F(z)}{C(z)}y(t) \implies \boxed{\hat y (t+k\mid t) = \frac{F(z)}{C(z)}y(t)}
\]
meaning that the optimal predictor from output is obtained as the output of a digital filter $F(z)/C(z)$ fed by $y(t)$ up to time $t$.

\textbf{Remark.}
The correct expression of the linear predictor can \emph{only} be obtained by using the canonical representation, otherwise there may be zeros of $C(z)$ which becomes unstable poles when reconstructing $e(t)$.

Let us illustrate this fact through an example.

\emph{Example. MA($1$)}
Let $e(t)\sim \WN(0,1)$.
\begin{align*}
	y(t) &= e(t) - 2 e(t-1)\\
	&= (1-2z^{-1} )e(t) &\text{non-canonical}\\
	&=\left( 1-\frac{1}{2} z^{-1}  \right) \cdot\frac{1-2z^{-1}}{1-\frac{1}{2} z^{-1}} e(t)\\
	&=\left( 1-\frac{1}{2} z^{-1}  \right) \xi(t) &\text{canonical}
\end{align*}
where $\xi(t)\sim \WN(0,4)$. The first form was non-canonical because there was a zero ($2$) outside the unit circle.

The process original process (non-canonical) can be written as
\[
	y(t+1)=\underbrace{e(t+1)}_{\substack{\text{unpred.}\\\text{at $t$}}}-\underbrace{2e(t)}_{\substack{\text{pred.}\\\text{at $t$}}}
\]
thus our optimal predictor from noise would be
\[
	\hat y^{e} (t+1\mid t) = -2e(t)
\]
Whereas the process in canonical form can be written as 
\[
	y(t+1) =\underbrace{\xi(t+1)}_{\substack{\text{unpred.}\\\text{at $t$}}}-\underbrace{\frac{1}{2} \xi(t)}_{\substack{\text{pred.}\\\text{at $t$}}}
\]
thus our optimal predictor from noise would be
\[
	\hat y^{\xi} (t+1\mid t)=-\frac{1}{2} \xi(t).
\]
The problem now is that if we try to reconstruct the noise from the output in the predictor $\hat y^{e} (t+1\mid t) = -2e(t)$ obtained by the non canonical form, we get:
\[
	y(t)=(1-2z^{-1})e(t) \iff e(t) \frac{1}{1-2z^{-1}} y(t) \implies \hat y^{e} (t+1\mid t) = -2e(t) = -\frac{2}{1-2z^{-1}}y(t)
\]
this is a \textbf{fatal mistake}, because it's not well defined, $e(t)$ cannot be reconstructed here!

The corret way is indeed using the canonical representation:
\[
	y(t) = \left( 1-\frac{1}{2} z^{-1}  \right) \xi(t) \iff \xi(t) = \frac{1}{1-\frac{1}{2} z^{-1} } y(t) \implies \boxed{\hat y^{\xi} (t+1\mid t)= -\frac{1}{2} \xi(t) = \frac{-\frac{1}{2} }{1-\frac{1}{2} z^{-1} } y(t)}
\]
If we consider the prediction error we see that they are different:
\[
	\E\left[ \left( y(t+1)-\hat y^{e}(t+1\mid t)  \right) ^2   \right]  =\E[e(t+1)^2 ]=1\neq 4=\E[\xi(t+1)^2 ]=\E\left[ \left( y(t+1)-\hat y ^{\xi}(t+1\mid t)  \right)^2    \right]  
\]
this means that $\hat y^{e} $ and $\hat y^{\xi} $ cannot be both the canonical predictor from output.

\textbf{Observation.}

\begin{align*}
	y(t+1)=e(t+1)-2e(t) \iff e(t)&=-\frac{1}{2} y(t+1)+\frac{1}{2} e(t+1)\\
	&= -\frac{1}{2} y(t+1)\underbrace{-\frac{1}{2} y(t+2)+\frac{1}{4}e(t+2)}_{\frac{1}{2} e(t+1)}\\
	&= -\frac{1}{2} y(t+1)-\frac{1}{4}y(t+2)-\frac{1}{8}y(t+3)+\frac{1}{8}e(t+3)\\
	&\quad\vdots
\end{align*}
This term is well-defined because the series converges, however we see that the noise of the non-canonical process cannot be reconstructed from the \emph{past} but from the \emph{future}! This is why it is not suitable to produce a predictor.

If instead we try to get something using values from the past we get a term which diverges:
\begin{align*}
	y(t)=e(t)-2e(t-1) \iff e(t)&=y(t)+2e(t-1)\\
	&=y(t)+\underbrace{2y(t-1)+2e(t-2)}_{2e(t-1)}\\
	&=y(t)+2y(t-1)+4y(t-2)+4e(t-3)\\
	&\quad\vdots
\end{align*}


\todo{saltone incredibile fino a}


\section{Optimal prediction of non zero mean ARMA}
\[
	y(t)=\frac{C(z)}{A(z)}e(t)\qquad e(t)\sim \WN(\mu,\lambda^2 )
\]
Hypothesis: canonical and minimum phase.
$\hat y(t+k\mid t)=?$
\[
	\E[e(t)]=\mu \implies \E[y(t)]=\frac{C(1)}{A(1)}\cdot\mu=m_{y} \quad \forall t
\]
We construct
\[
	\begin{cases}
		\tilde y(t)=y(t)-m_{y}\\
		\tilde e(t)=e(t)-\mu
	\end{cases}
	\implies 
	\tilde y(t)=\frac{C(z)}{A(z)}\tilde e(t) \quad \tilde e(t)\sim \WN(0,\lambda^2)
\]
$\tilde y$ is a zero mean ARMA
\[
	\hat{\tilde y} (t+k\mid t) = \frac{F(z)}{C(z)}\tilde y(t) \qquad \frac{C(z)}{A(z)}=E(z)+\frac{z^{-k}F(z) }{A(z)}
\]
So that we have
\[
	y(t)=\tilde y(t)+m_{y} \qquad y(t+k)=\tilde y(t+k)+\underbrace{m_{y}}_{\text{known}}
\]
\begin{align*}
	\hat y(t+k\mid t)&=\widehat{\tilde y(t+k)+m_{y}}\\
	&= \hat{\tilde y}(t+k\mid t)+m_{y}\\
	&=\frac{F(z)}{C(z)}\tilde y(t)+m_{y}\\
	&=\frac{F(z)}{C(z)}(y(t)-m_{y})+m_{y}\\
	&=\frac{F(z)}{C(z)}y(t)-\frac{F(z)}{C(z)}m_{y}+m_{y}\\
	&=\frac{F(z)}{C(z)}y(t)+\left( 1-\frac{F(1)}{C(1)} \right)  \cdot m_{y}
\end{align*}
\todo{passaggio non chiaro}

ARMA X
\[
	y(t)=\frac{B(z)}{A(z)} u(t-a)+\frac{C(z)}{A(z)}e(t)\qquad e(t)\sim \WN(0,\lambda^2 )
\]
available information:
\begin{gather*}
	y(t),y(t-1),\ldots \\
	u(t),u(t-1),\ldots
\end{gather*}































