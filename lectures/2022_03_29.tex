%!TEX root = ../main.tex

\section{Identification of ARMA/ARMAX models (Maximum Likelihood (ML) method)}

We consider a generic ARMAX:
\[
	M(\vartheta ): \quad y(t)=\frac{B(z,\vartheta )}{A(z,\vartheta)} u(t-d)+\frac{C(z,\vartheta )}{A(z,\vartheta)}e(t)\quad e(t)\sim \WN(0,\lambda^2)
\]
where
\begin{align*}
	A(z)&=1-a_{1} z^{-1}-a_{2} z^{-2}-\cdots-a_{m} z^{-m} \\
	B(z)&=b_{0}+b_{1} z^{-1}+b_{2} z^{-2}+\cdots+b_{p} z^{-p} \\
	C(z)&=1+c_{1} z^{-1}+c_{2} z^{-2}+\cdots+c_{n} z^{-n}
\end{align*}
We assume that $C(z,\vartheta)\neq 1$. Since $A$ and $C$ are monic the first term of the long division always gives $1$ and the remainder is always $C-A$. The predictor is then:
\[
	\hat{M}(\vartheta): \quad \hat{y}(t \mid t-1, \vartheta)=\frac{C(z,\vartheta)-A(z,\vartheta)}{C(z,\vartheta)} y(t)+\frac{B(z,\vartheta)}{C(z,\vartheta)} u(t-d)
\]
And the prediction error is:
\begin{align*}
	\varepsilon(t, \vartheta)=y(t)-\hat{y}(t \mid t-1, \vartheta)&=\left[1-\frac{C(z,\vartheta)-A(z,\vartheta)}{C(z,\vartheta)}\right] y(t)-\frac{B(z,\vartheta)}{C(z,\vartheta)} u(t-d)\\
	&=\frac{A(z,\vartheta)}{C(z,\vartheta)} y(t)-\frac{B(z,\vartheta)}{C(z,\vartheta)} u(t-d)
\end{align*}
Since we have $C(z,\vartheta)$ in the denominator, the error is \emph{no more linear} in $\vartheta$.
The cost function is \emph{non-convex} and may present \emph{local} minima:
\[
	J_{N}(\vartheta)=\frac{1}{N} \sum_{t=1}^{N} \varepsilon(t, \vartheta)^{2}
\]
To tackle the non-linearity we can use some kinds of numerical optimization using descent methods:
\begin{itemize}
	\item the algorithm is initialized with an initial estimate (typically randomly chosen) of the optimal parameter vector: $\vartheta^{0}$;
	\item update rule: $\vartheta^{i+1} = f (\vartheta^{i})$;
	\item the sequence of estimates should converge to $\hat\vartheta_{N}$.
\end{itemize}
If there are local minima, we can just use an \emph{empirical} approach by running the algorithm with many different starting values getting a range of candidates of minimum values, hoping to explore the entire domain and not miss the global minimum. This of course comes with the cost of computational complexity.


\section{Asymptotic Analysis of PEM Identification}

Is $M(\hat\vartheta_{N})$ a good model for the process $y(t)$?
We can give an asymptotic answer as $N\to \infty$

\textbf{Assumption on the data generating system:}

$y(t),u(t)$ are \gls{ssp} generated by a \textbf{linear system:}\footnote{Not necessarily of the same type as in $M$.}
\[
	S:
	\begin{cases}
		y(t) = G(z)u(t) + H(z)e(t)\\
		u(t) = F(z)r(t) + S(z)e(t)
	\end{cases}
	\qquad
	\begin{array}{l}
		e(t)\sim \WN(0,\lambda^2)\\
		r(t)\sim \WN(0,\sigma^2)
	\end{array}
\]
where $G(z),H(z),F(z),S(z)$ are \textbf{asymptotically stable, rational} transfer functions.

The most typical cases are when:
\begin{itemize}
	\item $S(z)=0$ (\textbf{open-loop experiment})
	\fg{0.6}{Screen Shot 2022-04-02 at 10.44.03}
	\item $S(z)\neq 0$ (\textbf{closed-loop experiment}), $S(z)$ accounts for $u(t)$ depending on $e(t)$ because of the feedback.
	\fg{0.6}{Screen Shot 2022-04-02 at 10.45.22}
\end{itemize}
The measured data sequence corresponds to a particular \textbf{realization} of input/output signals of $S$:
\[
	D^{N}=
	\begin{cases}
	 	u(1),u(2),\ldots,u(N)\\
	 	y(1),y(2),\ldots,y(N)
	\end{cases}
	\quad
	\text{to be thought as}
	\quad
	\begin{cases}
	 	u(1,\overline{s}),u(2,\overline{s}),\ldots,u(N,\overline{s})\\
	 	y(1,\overline{s}),y(2,\overline{s}),\ldots,y(N,\overline{s})
	\end{cases}
\]
Hence also the predictor computed from the given realization should be thought as:
\[
	\hat{y}(i\mid i-1,\vartheta) = f(D^{N}) = \hat{y}(i\mid i-1,\vartheta,\overline{s})
\]
and also:
\begin{gather*}
	\varepsilon(i,\vartheta) = y(i,\overline{s}) - \hat{y}(i\mid i-1,\vartheta,\overline{s}) = \varepsilon(i,\vartheta,\overline{s})
	\quad
	J_{N}(\vartheta) = \frac{1}{N}\sum_{i=1}^{N} \varepsilon(i,\vartheta,\overline{s})^2 = J_{N}(\vartheta,\overline{s})\\
	\hat{\vartheta}_{N} = \argmin_{\vartheta} J_{N}(\vartheta,\overline{s}) = \hat{\vartheta}_{N}(\overline{s})
\end{gather*}
\fg{0.6}{Screen Shot 2022-04-02 at 11.01.06}
Depending on the experiment, different costs and minimizers. As $N\to \infty$, the sequence of minimizers \textbf{converges:}\footnote{In practice, a good number could be $N\geq 300$.}
\fg{0.6}{Screen Shot 2022-04-02 at 11.04.00}
Indeed we have the following result.
\begin{theorem}
	Under the current assumptions, as the number of data points becomes larger and larger, we have with probability one that:
	\[
		J_{N}(\vartheta,s) = \frac{1}{N}\sum_{i=1}^{N} \varepsilon(i,\vartheta,s)^2 \xrightarrow{N\to\infty} \E[\varepsilon(t,\vartheta,s)^2] = \overline{J}(\vartheta)
	\]
	The convergence is almost sure w.r.t. $s$, uniform w.r.t. $\vartheta$ (the error goes to $0$ with the same rate for all $\vartheta$).\\
	Moreover if we define the \textbf{set of all minimizers:}
	\[
		\Delta = \left\{ \vartheta ^{\star} : \overline{J}(\vartheta ^{\star}\le \overline{J}(\vartheta),\forall \vartheta  \right\}
	\]
	we have:
	\[
		\text{distance}[\hat{\vartheta}_{N}(s), \Delta]\xrightarrow{N\to\infty} 0
	\]
\end{theorem}
As a corollary we have that if $\Delta ={\vartheta ^{\star} }$ (i.e. $J_{N}(\vartheta)$ has a unique minimum point), then:
\[
	\hat{\vartheta}_{N}(s) \xrightarrow{N\to\infty} \vartheta ^{\star} 
\]
almost surely and regardless of the experiment.

To sum up, as $N$ is large enough we can approximate $M(\hat{\vartheta}(s)\approx M(\vartheta ^{\star}$.

Question: is $M(\vartheta^{\star}$ satisfactory? Let's assume that the systeam we are dealing with belongs to the class in which we construct the model, $S\in \mathcal{M}$. This is an ideal situation, we have enough degrees of freedom to describe the mechanism by which $y(t)$ is generated. That means that $\exists\vartheta^{0}: M(\vartheta^{0}) = S$.