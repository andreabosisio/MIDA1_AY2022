%!TEX root = ../main.tex
Now, let us consider the stochastic processes $y(t)$ obtained as output of an asymptotically stable digital filter $F(z)$ fed by a stationary stochastic process $v(t)$ as input, but with a generic initialization (not steady-state output).

\fg{0.4}{Screenshot (17)}

\begin{theorem}
	There is just one stationary output which corresponds to the steady-state solution. However, if $F(z)$ is asymptotically stable, then all possible outputs obtained for different initialization of the digital filter $F(z)$ tends asymptotically (as $t \rightarrow \infty$) to the steady-state solution, i.e. to the stationary output.
\end{theorem}

\fg{0.7}{Screenshot (18)}

\section{Weak(Wide sense) characterization of AR,ARMA processes}
\textbf{Goal.} Given an AR, ARMA process compute the mean $m_y$ and covariance function $\gamma_y(\tau)$.

%Since the steady-state solution is an MA($\infty$) process we could use such results but are too diffucult

We'll rely on the recursive equation characterizing such processes.

\subsection{AR processes}
\textbf{Example.}

Let us consider the AR($1$) (or equivalently ARMA($1,0$)) process generated according to:
\begin{align*}
	y(t)=a \cdot y(t-1)+e(t) \quad \text{where} \quad e(t) \sim W N\left(0, \lambda^{2}\right)
\end{align*}

\begin{itemize}
	\item Is $y(t)$ stationary?
	\item Compute $m_{y}$ and $\gamma_{y}(\tau)$ for $\tau=0, \pm 1, \pm 2, \ldots$
\end{itemize}

Operatorial representation for $y(t)$:

\begin{align*}
	y(t)&=z^{-1} a y(t)+e(t) \\
	\left(1-z^{-1} a\right) y(t)&=e(t) \\
	y(t)&=\frac{1}{1-z^{-1} a} e(t)
\end{align*}

The transfer function with positive powers (to identify zeroes and poles) is $y(t)=\frac{z}{z-a} e(t) \quad$.

There is just one pole: $z=a .$

The process generating system is asymptotically stable if $|a| <1$. 

Since $e(t)$ is a S.S.P. (by definition of white noise), when $|a| <1$ the steady-state output process $y(t)$ is a S.S.P.

\textbf{Mean.}
Start from the time-domain representation and apply expectation to both sides:
\[
	\mathbb{E}[y(t)]=\mathbb{E}[a \cdot y(t-1)+e(t)] \implies \mathbb{E}[y(t)]=a \cdot \mathbb{E}[y(t-1)]+\mathbb{E}[e(t)]
\]
Thanks to stationarity $\mathbb{E}[y(t)]=\mathbb{E}[y(t-1)]=m_{y}$, so that $m_{y}=a \cdot m_{y}+m_{e}$.
Then:
$$
m_{e}=0 \implies  m_{y}=0
$$
\textbf{Variance.}
Let us compute
\[
	\gamma_{y}(0)=\mathbb{E}\left[\left(y(t)-m_{y}\right)^{2}\right]
\]
Remember that $m_{y}=0$. Start from $y(t)=a \cdot y(t-1)+e(t)$, take the square and apply operator $\mathbb{E}[\cdot]$ to both side:
\begin{align*}
	\gamma_{y}(0)&=\mathbb{E}\left[(y(t))^{2}\right]\\
	&=\mathbb{E}\left[(a \cdot y(t-1)+e(t))^{2}\right]\\
	&=a^{2} \mathbb{E}\left[y(t-1)^{2}\right]+\mathbb{E}\left[e(t)^{2}\right]+2 a \mathbb{E}[y(t-1) e(t)]\\
\end{align*}
Mid-terms evaluation:
\[
	2 a \mathbb{E}[y(t-1) e(t)]=0 \qquad \text{(we will show this later)}
\]
Indeed, by using the MA($\infty$) representation for $y(t-1)$ (AR process):
$$
y(t-1)=e(t-1)+a \cdot e(t-2)+a^{2} \cdot e(t-3)+a^{3} \cdot e(t-4)+\cdots
$$
we have that:
\[
	\mathbb{E}[e(t) y(t-1)]=\mathbb{E}\left[e(t) \cdot\left(e(t-1)+a \cdot e(t-2)+a^{2} \cdot e(t-3)+a^{3} \cdot e(t-4)+\cdots\right)\right]=0
\]
(all products give null contribution).


$\mathbb{E}\left[(y(t-1))^{2}\right]=\gamma_{y}(0)$ (thanks to stationarity)

$\mathbb{E}\left[(e(t))^{2}\right]=\lambda^{2}$

Hence,
\[
	\gamma_{y}(0)=a^{2} \gamma_{y}(0)+\lambda^{2} \implies \gamma_{y}(0)=\frac{\lambda^{2}}{1-a^{2}}
\]
\textbf{Covariance.}
Remember that $m_{y}=0$ and substitute $y(t)=a \cdot y(t-1)+e(t)$,
\begin{align*}
	\gamma_{y}(1)&=\mathbb{E}\left[\left(y(t)-m_{y}\right) \cdot\left(y(t-1)-m_{y}\right)\right]\\
	&=\mathbb{E}[y(t) y(t-1)]\\
	&=\mathbb{E}[(a \cdot y(t-1)+e(t))y(t-1)]\\
	&=a \cdot \mathbb{E}\left[(y(t-1))^{2}\right]+\mathbb{E}[e(t) y(t-1)]
\end{align*}

We already know that $\mathbb{E}\left[(y(t-1))^{2}\right]=\gamma_{y}(0)$, while as before $\mathbb{E}[e(t) y(t-1)]=0$.
$$
\gamma_{y}(1)=a \cdot \gamma_{y}(0)=a \cdot \frac{\lambda^{2}}{1-a^{2}}
$$
Arguing the same way,
\[
	\gamma_{y}(2)=a \cdot \gamma_{y}(1)=a^{2} \cdot \frac{\lambda^{2}}{1-a^{2}}
\]
Summary:
\begin{equation*}
	\begin{cases}
		\gamma_{y}(0)=\frac{\lambda^{2}}{1-a^{2}} \\
		\gamma_{y}(1)=\gamma_{y}(-1)=a \cdot \gamma_{y}(0) \\
		\gamma_{y}(2)=\gamma_{y}(-2)=a \cdot \gamma_{y}(1) \\
		\vdots
	\end{cases}
\end{equation*}

Recursive expression for $\gamma_{y}(\tau)$
$$
	\gamma_{y}(\tau)=a^{\tau} \cdot \frac{\lambda^{2}}{1-a^{2}}
$$
This result has been established for a generic AR($1$) process.
Those equations are called \emph{Yule-Walker equations}.

Grafical representation:

\fg{0.7}{Screenshot (14)}

\fg{0.7}{Screenshot (16)}

\subsection{ARMA processes}
$$
y(t)=a_{1} y(t-1)+\cdots+a_{m} y(t-m)+c_{0} e(t)+\cdots+c_{n} e(t-n)
$$
where $e(t) \sim W N\left(0, \lambda^{2}\right)$.

\textbf{Mean.}
\begin{align*}
	\mathbb{E}[y(t)] &=\mathbb{E}\left[a_{1} y(t-1)+\cdots+a_{m} y(t-m)+c_{0} e(t)+\cdots+c_{n} e(t-n)\right] \\
	&=a_{1} \mathbb{E}[y(t-1)]+\cdots+a_{m} \mathbb{E}[y(t-m)]+c_{0} \mathbb{E}[e(t)]+\cdots+c_{n} \mathbb{E}[e(t-n)]
\end{align*}
$$
m_{y}=a_{1} m_{y}+\cdots+a_{m} m_{y}+c_{0} \cdot 0+\cdots+c_{n} \cdot 0
$$

By asintotical stability we can prove that $(1-a_1-\cdots-a_m)\neq0$, i.e. $m_{y}=0$.

\textbf{Covariance.}
\begin{align*}
	\gamma_{y}(0) = \mathbb{E}\left[y(t)^{2}\right]&=\mathbb{E}\left[\left(a_{1} y(t-1)+\cdots+a_{m} y(t-m)+c_{0} e(t)+\cdots+c_{n} e(t-n)\right)^{2}\right]=\\
	&= a_{1}^{2} \mathbb{E}\left[y(t-1)^{2}\right]+a_{2}^{2} \mathbb{E}\left[y(t-2)^{2}\right]+2 a_{1} a_{2} \mathbb{E}[y(t-1) y(t-2)]+\cdots \\
	&\qquad+c_{0}^{2} \mathbb{E}\left[e(t)^{2}\right]+2 a_{1} c_{0} \mathbb{E}[y(t-1) e(t)]+\cdots
\end{align*}

Hence
$$
\gamma_{y}(0)=a_{1}^{2} \gamma_{y}(0)+a_{2}^{2} \gamma_{y}(0)+2 a_{1} a_{2} \gamma_{y}(1)+\cdots
$$

Then
\begin{align*}
	\mathbb{E}[y(t) y(t-1)]&=\mathbb{E}\left[\left(a_{1} y(t-1)+\cdots+a_{m} y(t-m)+c_{0} e(t)+\cdots+c_{n} e(t-n)\right) y(t-1)\right]= \\
	&=a_{1} \mathbb{E}\left[y(t-1)^{2}\right]+\cdots+c_{0} \mathbb{E}[e(t) y(t-1)]+\cdots
\end{align*}

Proceeding this way:
$$
\begin{cases}
	\gamma_{y}(0)=a_{1}^{2} \gamma_{y}(0)+a_{2}^{2} \gamma_{y}(0)+2 a_{1} a_{2} \gamma \\
	\gamma_{y}(1)=a_{1} \gamma_{y}(0)+\cdots+c_{0} \mathbb{E}[e(t) y(t-1)] \\
	\vdots \\
	\gamma_{y}(m-1)=a_{1} \gamma_{y}(m-2)+\cdots
\end{cases}
$$

$m$ variables and $m$ linear equations (\emph{Yule-walker equations} for an ARMA process).

Then, $\gamma_{y}(m), \gamma_{y}(m+1), \ldots$ can be recursevely computed from
$$
\gamma_{y}(0), \gamma_{y}(1), \ldots, \gamma_{y}(m-1)
$$
\begin{align*}
	\gamma_{y}(m)&=\mathbb{E}[y(t) y(t-m)] \\
	&=\mathbb{E}\left[\left(a_{1} y(t-1)+\ldots+a_{m} y(t-m)+c_{0} e(t)+\ldots+c_{n} e(t-n)\right) y(t-m)\right]=\ldots
\end{align*}