%!TEX root = ../main.tex

\begin{theorem}{Spectral Factorization}
	Let $y(t)$ be a \gls{ssp} with rational spectral density. 
	Then, there exists an unique \gls{wn} process $\xi(t)$ with suitable mean and variance and an unique rational transfer function $W(z)$ such that:
	\begin{align*}
		y(t) = W(z)\xi(t)
	\end{align*}
    and, ($C(z)$ and $A(z)$ are the numerator and denominator of $W(z)$):
    \begin{itemize}
		\item $C(z)$ and $A(z)$ are monic (i.e. the coefficients of the maximum degree terms of $C(z)$ and $A(z)$ are equal to 1)
		\item $C(z)$ and $A(z)$ have null relative degree
		\item $C(z)$ and $A(z)$ are coprime (i.e. they have no common factors) 
		\item the absolute value of the poles and the zeroes of $W(z)$ is less than or equal to 1 (i.e. poles and zeroes are inside the unit circle)
    \end{itemize}
\end{theorem}

When all the four conditions above are satisfied, we will say that $y(t) = W(z)\xi(t)$ is a canonical representation of $y(t)$.

\textbf{Observation.} Conditions 1, 2, and 3 remove any ambiguity as due to the process described in Case 1, Case 2, and Case 3, respectively. 

The first part of Condition 4 assure that $W(z)$ is asymptotically stable so that $y(t)$ is well defined, while the second part removes any ambiguity as due to the process described in Case 4.

%ci sarebbe da fare un esempio lungo da zero
\textbf{Example.}

\begin{align*}
	y(t)=\frac{2+5z^{-1}+2z^{-2}}{z^2+\frac{5}{6}z+\frac{1}{6}}\cdot e(t)  \qquad \text{where } e(t)\sim \WN(0,1)
\end{align*}
$W(z)$ is not monic so we can proceed modifying the input in order to have a transfer function that satisfies all the above properties.
\begin{align*}
	y(t)&=\frac{2+5z^{-1}+2z^{-2}}{z^2+\frac{5}{6}z+\frac{1}{6}}\cdot e(t) \\
	&=\frac{2+5z^{-1}+2z^{-2}}{z^2+\frac{5}{6}z+\frac{1}{6}}\cdot\frac{z^2}{2}\cdot\frac{2}{z^2}\cdot e(t)\\
	&=\frac{z^2+\frac{5}{2}z+1}{z^2+\frac{5}{6}z+\frac{1}{6}}\cdot \eta(t)
\end{align*}
where $\eta(t)=2z^{-2}e(t)=2e(t-2)\sim \WN(2\cdot0,4\cdot 1)\sim \WN(0,4)$.

\begin{align*}
	&\text{zeros:}\quad z^2+\frac{5}{2}z+1=(z+2)(z+\frac{1}{2}) \implies z_{1,2}=-2,-\frac{1}{2}.\\
	&\text{poles:}\quad z^2+\frac{5}{6}z+\frac{1}{6}=(z+\frac{1}{3})(z+\frac{1}{2}) \implies z_{1,2}=-\frac{1}{3},-\frac{1}{2}.
\end{align*}



Then:
\begin{align*}
	y(t)=\frac{(z+2)\cancel{(z+\frac{1}{2})}}{(z+\frac{1}{3})\cancel{(z+\frac{1}{2})}}\cdot \eta(t) 
\end{align*}
Since numerator and denominator are not coprime, we can proceed as follows:
\begin{align*}
	y(t)=\frac{(z+\frac{1}{2})}{(z+\frac{1}{3})}\left[\frac{z+2}{(z+\frac{1}{2})}\eta(t)\right]=\frac{(z+\frac{1}{2})}{(z+\frac{1}{3})}\cdot \xi(t)
\end{align*}
where $\xi(t)=\frac{z+2}{(z+\frac{1}{2})}\eta(t)\sim \WN(0,4\cdot2^2)\sim \WN(0,16)$.

Finally we reduce our problem in a Canonical form.
% c'Ã¨ un ultima parte che non capisco rip
\section{Linear Optimal prediction theory (for ARMA processes) }
Let us consider a zero mean ARMA process:
\[
	y(t)=W(z)e(t) \quad \text{where }e(t)\sim \WN(0,\lambda^{2} )
\]
$W(z)=\frac{C(z)}{A(z)}$ is an asymptotically stable rational transfer function $\implies$ $y(t)$ is well defined and a \gls{ssp}.

\textbf{Further Assumptions:}
\begin{itemize}
	\item $y(t) =W(z)e(t)$ is a canonical representation.
	\item $W(z)$ has no zeroes on the unit circle boundary (i.e. no zeroes with absolute value equal to 1)
\end{itemize}
 
\textbf{Observations:}
First assumption is not much a hurdle since every ARMA process admits a canonical representation, and if $y(t)$ is not given in its canonical representation one can easily reconstruct it. Why first assumption is required will be clear later. 

The second one is a limitating assumption (mild) that exclude models belonging to a small subclass that can be approximated by other ARMA.

\textbf{Problem} ($k$-step prediction.)

Given the observations of the process $y(t)$ up to time $t$:
$$
	\ldots , y(t-101), y(t-100), \ldots , y(t-2), y(t-1), y(t)
$$
predict the future value of the process at time $t + k$.


A predictor is any function of the available information which is used to guess the future value of the process.

$\hat{y}(t + k | t) =$ predictor of $y(t + k)$ given the observations up to time $t$.

In general: 
$$\hat{y}(t + k | t) = f ( y(t), y(t-1), y(t-2),\ldots)$$
i.e. the predictor is any function of the available observations of the process $y(t)$.

\textbf{Abstraction.} we suppose to have the whole observations from $-\infty$ up to $t$ (infinite sequence of observations).

Then we can write:
\begin{align*}
	\hat{y}(t + k | t)=\sum_{i=0}^{\infty}\alpha_i y(t-i)
\end{align*}
where ${\{\alpha_i\}}_{i=0}^\infty$ are parameters to be selected in order to achieve the best result.

\textbf{Intuitive Goal.} 
We want a predictor which is good w.r.t $S$ (of the specific experiment).
$$\hat{y}(t + k | t)\sim y(t+k) $$

We need to quantify this intuitive closeness concept be rewritten in some rigorous mathematical notion of distance valid for random variables.

\begin{definition}[\Gls{mse} of prediction]
	\[
		\mathbb{E}[(y(t+k,s)-\hat{y}(t+k|t,s))^2]
	\]
\end{definition}

\textbf{Goal:} choose $\alpha_0,\ldots,\alpha_i,\ldots$ in order to minimize the \gls{mse}.